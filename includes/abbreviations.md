*[devlin2019bert]: Devlin, Jacob et al. 2019 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
*[vaswani2017attention]: Vaswani, Ashish et al. 2017 'Attention Is All You Need'
*[practical_guide_to_rnn_and_lstm]: Mayank, Mohit et al.  'A practical guide to RNN and LSTM in Keras | by Mohit Mayank | Towards Data Science'
*[Guide_to_custom_recurrent_modeling]: Mayank, Mohit et al.  'Guide to Custom Recurrent Modeling in Keras | by Mohit Mayank | Towards Data Science'
*[the_illustrated_bert]: jalammar et al.  'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)'
*[rdf_primer]: w3 et al.  'RDF 1.1 Primer'
*[wikidata_sparql_query_helper]: wikidata et al.  'Wikidata SPARQL Query Helper'
*[wikidata_api_services]: wikidata et al.  'Wikidata API Services'
*[guo2020survey]: Guo, Qingyu et al. 2020 'A Survey on Knowledge Graph-Based Recommender Systems'
*[färber2018knowledge]: Färber, Michael et al. 2018 'Which Knowledge Graph Is Best for Me?'
*[Ji_2021]: Ji, Shaoxiong et al. 2021 'A Survey on Knowledge Graphs: Representation, Acquisition, and Applications'
*[lstm_understanding]: Olah, Christopher et al.  'Understanding LSTM Networks'
*[raffel2020exploring]: Raffel, Colin et al. 2020 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer'
*[radford2018improving]: Radford, Alec et al. 2018 'Improving language understanding by generative pre-training'
*[radford2019language]: Radford, Alec et al. 2019 'Language models are unsupervised multitask learners'
*[brown2020language]: Brown, Tom B et al. 2020 'Language models are few-shot learners'
*[roberts2020knowledge]: Roberts, Adam et al. 2020 'How Much Knowledge Can You Pack Into the Parameters of a Language Model?'
*[fullstackdeeplearning]: Karayev, Sergey et al. Full Stack Deep Learning Course (https://fullstackdeeplearning.com/)
