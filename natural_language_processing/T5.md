T5
=========

## Introduction

- T5 stands for "Text-to-Text Transfer Transformer". It was released by Google on 2020. As the name suggests, it's a tranformer based language model used for text generation. For more details about other text generation models, refer the [Text Generation](text_generation.md) chapter. 
- In contrast to other famous Transformer based models like BERT or GPT, which is made up of either the encoder or decoder part of the Transformer, T5 paper showcase that using the complete encoder-decoder architecture is better than only using decoder. Apart from this, the paper also curated and released  Colossal Clean Crawled Corpus (C4) -  a huge crawled and cleaned dataset for pre-training language model using self-supervised learning. {cite}`raffel2020exploring` 

```{figure} /imgs/nlp_transformers.png
---
height: 400px
---
Transformer architecture. Left part is the encoder, right part is the decoder. T5's architecture is very similar to this one. {cite}`vaswani2017attention` {cite}`raffel2020exploring` 
```

- Some example tasks that can be performed using T5 is shown below, 

```{figure} /imgs/t5_example.gif
---
height: 200px
---
T5 text-to-text framework examples. See: [Google Blog](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
```
## Code

<!-- ### T5 inference

```{code-block} python
---
lineno-start: 1
---
``` -->

### T5 finetuning

- Tweet sentiment data can be downloaded from [here](https://www.kaggle.com/kazanova/sentiment140)
- Some differences from training other text generation models (due to the [SimpleT5](https://github.com/Shivanandroy/simpleT5) package),
    - We don't need the `Dataset` class, as SimpleT5 works directly on pandas dataframe. Hence we load the data, do some initial pre-processing, split the data and return the pandas dataframe. *(no need to tokenize, create Dataset class, isn't this great!?)*
    - One more point to note is that we do not need to create prompt formats for this package. This way we can separate out the input tweet and sentiment label into different columns, here `source_text` and `target_text`, respectively *(Line 29 and 30)*.

```{code-block} python
---
lineno-start: 1
---
# import
import pandas as pd
from simplet5 import SimpleT5
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split

# Data loading
# -------

# Data load function
def load_sentiment_dataset(random_seed = 1):
    # load dataset and sample 10k reviews.
    file_path="../input/sentiment140/training.1600000.processed.noemoticon.csv"
    df = pd.read_csv(file_path, encoding='ISO-8859-1', header=None)
    df = df[[0, 5]]
    df.columns = ['label', 'text']
    df = df.sample(10000, random_state=1)
    
    # modify the label  
    map_label = {0:'negative', 4: 'positive'}
    df['label'] = df['label'].apply(lambda x: map_label[x])
    
    # divide into test and train
    X_train, X_test, y_train, y_test = \
              train_test_split(df['text'].tolist(), df['label'].tolist(),
              shuffle=True, test_size=0.05, random_state=random_seed, stratify=df['label'])
    
    # transform train and test data into pandas dataframe
    train_data = pd.DataFrame({'source_text': X_train, 'target_text': y_train})    
    test_data = pd.DataFrame({'source_text': X_test, 'target_text': y_test})    

    # return
    return train_data, test_data

# load
train_df, test_df = load_sentiment_dataset()  

# Train
# -------

# load model
model = SimpleT5()
model.from_pretrained(model_type="t5", model_name="t5-base")

# train model
model.train(train_df=train_df,
            eval_df=test_df, 
            source_max_token_len=300, 
            target_max_token_len=200, 
            batch_size=8, 
            max_epochs=2, 
            outputdir = "outputs",
            use_gpu=True
            )

# Test
# -------

# load the best model
last_epoch_model = "..." # put the name here
model.load_model("t5", last_epoch_model, use_gpu=True)

# for each test data perform prediction
predictions = []
for index, row in test_df.iterrows():
    prediction = model.predict(row['source_text'])[0]
    predictions.append(prediction)

# computer performance
df = test_df.copy()
df['predicted_label'] = predictions
df['original_label'] = df['target_text']
print(f1_score(df['original_label'], df['predicted_label'], average='macro'))
```

## Additional materials

- Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer - [Link](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)


## References

```{bibliography}
:filter: docname in docnames
```