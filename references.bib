@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vaswani2017attention,
      title={Attention Is All You Need},
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{practical_guide_to_rnn_and_lstm,
  author = {Mohit Mayank},
  title = {A practical guide to RNN and LSTM in Keras | by Mohit Mayank | Towards Data Science},
  howpublished = {\url{https://towardsdatascience.com/a-practical-guide-to-rnn-and-lstm-in-keras-980f176271bc}},
  month = {},
  year = {},
  note = {(Accessed on 06/22/2021)}
}

@misc{Guide_to_custom_recurrent_modeling,
  author = {Mohit Mayank},
  title = {Guide to Custom Recurrent Modeling in Keras | by Mohit Mayank | Towards Data Science},
  howpublished = {\url{https://towardsdatascience.com/guide-to-custom-recurrent-modeling-in-keras-29027e3f8465}},
  month = {},
  year = {},
  note = {(Accessed on 06/22/2021)}
}

@misc{the_illustrated_bert,
	title={The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)},
	howpublished={\url{http://jalammar.github.io/illustrated-bert/}},
  note = {Accessed: 2021-05-06}
}
